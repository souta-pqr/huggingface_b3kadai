{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face LLM利用演習\n",
    "\n",
    "Hugging Faceで提供されている大規模言語モデル（LLM）の利用方法について記載されたノートブックです。\n",
    "\n",
    "## 目次\n",
    "1. 環境構築\n",
    "2. Transformersライブラリの基本\n",
    "3. テキスト生成モデルの利用\n",
    "4. 多言語モデルの利用\n",
    "5. ファインチューニング\n",
    "6. Hugging Face Hubとの連携"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境構築\n",
    "\n",
    "まずは必要なライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: sentencepiece in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (0.2.0)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Using cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.0 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install transformers datasets evaluate sentencepiece scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (27 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.0%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.0%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: fsspec in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (from torchvision) (2.2.5)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl (176.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchvision-0.22.0%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.0%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, pillow, networkx, MarkupSafe, jinja2, torch, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [torchaudio]9\u001b[0m [torchaudio]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-2.1.5 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 pillow-11.0.0 sympy-1.13.3 torch-2.7.0+cpu torchaudio-2.7.0+cpu torchvision-0.22.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# PyTorchをインストール\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /mnt/data1/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# オプション: 進捗バーの表示のためにtqdmをインストール\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "インストールしたライブラリのバージョンを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.51.3\n",
      "PyTorch version: 2.7.0+cpu\n",
      "Datasets version: 3.6.0\n",
      "GPU available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "\n",
    "# GPUを無効化する\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "torch.cuda.is_available = lambda: False\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformersライブラリの基本\n",
    "\n",
    "Hugging Face Transformersライブラリの基本的な使い方を学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9992625117301941}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# pipelineの使用例（テキスト分類）\n",
    "classifier = pipeline(\"sentiment-analysis\", device=\"cpu\")\n",
    "result = classifier(\"I love using Hugging Face models!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本的なパイプライン\n",
    "\n",
    "Transformersライブラリには様々なタスク用のパイプラインが用意されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hugging Face is a new feature in the \"Skins of the Web.\"\\n\\n\\nThe feature allows you to hide the subject in a photo'}, {'generated_text': 'Hugging Face is the first book ever written by Jonathan Wilshere.\\n\\nThe cover story was given by one of the best minds in London'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9182092547416687, 'start': 49, 'end': 67, 'answer': 'New York and Paris'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Hugging Face is an AI community and platform where researchers, data scientists,machine learning engineers, and developers can collaborate on machine learning projects. It provides tools for building, training, and deploying machine learning models.'}]\n"
     ]
    }
   ],
   "source": [
    "# テキスト生成\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\", device=\"cpu\")\n",
    "text = generator(\"Hugging Face is\", max_length=30, num_return_sequences=2)\n",
    "print(text)\n",
    "\n",
    "# 質問応答\n",
    "qa = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", device=\"cpu\")\n",
    "context = \"Hugging Face was founded in 2016 and is based in New York and Paris.\"\n",
    "result = qa(question=\"Where is Hugging Face based?\", context=context)\n",
    "print(result)\n",
    "\n",
    "# 要約\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=\"cpu\")\n",
    "long_text = \"\"\"\n",
    "Hugging Face is an AI community and platform where researchers, data scientists,\n",
    "machine learning engineers, and developers can collaborate on machine learning projects.\n",
    "It provides tools for building, training, and deploying machine learning models.\n",
    "The company also maintains a popular repository of pre-trained models that can be used\n",
    "for a wide range of tasks including natural language processing, computer vision, and audio processing.\n",
    "\"\"\"\n",
    "summary = summarizer(long_text, max_length=50, min_length=10, batch_size=1)  # バッチサイズを小さく\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. テキスト生成モデルの利用\n",
    "\n",
    "特に大規模言語モデル（LLM）を使ったテキスト生成に焦点を当てます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kobori/.conda/envs/b3study/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world where AI has become ubiquitous, vernacular, and the world of AI is becoming increasingly difficult to understand.\n",
      "\n",
      "The world is now a globalized world, with the rise of the Internet, the proliferation of social media, social networks, etc. The world has been transformed into a digital world. It is a new world with a lot of new technologies, new technology, a whole new way of thinking, more than just a few new ideas. And it is not just the\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 軽量なGPT-2モデルを使用\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# トークナイザーとモデルのロード\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cpu\")\n",
    "\n",
    "# テキスト生成の詳細なコントロール\n",
    "prompt = \"In a world where AI has become ubiquitous, \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "# 生成パラメータを設定\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,  # 創造性の制御（高いほど多様な出力）\n",
    "    top_p=0.9,        # 核サンプリング\n",
    "    no_repeat_ngram_size=2  # 同じフレーズの繰り返しを防止\n",
    ")\n",
    "\n",
    "# デコードして表示\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### より効率的な方法\n",
    "\n",
    "モデルを実行する際の最適化手法を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading or using the model: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`\n",
      "\n",
      "Trying with simpler approach...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about a robot that learns to feel emotions.\n",
      "Robot:  I'm a robot, my life is full of joy. I'm happy for you, and I love you, and I want to be your friend.\n",
      "Robot:  I'm a robot, my life is full of joy. I'm happy for you, and I love you, and I want to be your friend.\n",
      "Robot:  I'm a robot, my life is full of joy. I'm happy for you, and I love you, and I want to be your friend.\n",
      "Robot:  I'm a robot, my life is full of joy. I'm happy for you, and I\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 小型の言語モデル「gpt2」を使用\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "try:\n",
    "    # トークナイザーのロード\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # モデルのロード\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        device_map=\"cpu\",\n",
    "        load_in_8bit=True \n",
    "    )\n",
    "    \n",
    "    # プロンプトの設定\n",
    "    prompt = \"\"\"Write a short story about a robot that learns to feel emotions.\n",
    "    Robot: \"\"\"\n",
    "    \n",
    "    # 入力のトークン化\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # テキスト生成\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=200,  # 短くして処理を高速化\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # 生成されたテキストのデコード\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading or using the model: {e}\")\n",
    "    print(\"\\nTrying with simpler approach...\")\n",
    "    \n",
    "    # 量子化なしの標準的なアプローチ\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cpu\")\n",
    "    \n",
    "    prompt = \"Write a short story about a robot that learns to feel emotions.\\nRobot: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=150, \n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 多言語モデルの利用\n",
    "\n",
    "日本語を含む多言語モデルを使用する方法を紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本の四季について短い文章を書いてください。\n",
      "\n",
      "1. 四季\n",
      "\n",
      "2. 季節\n",
      "\n",
      "3. 季節感\n",
      "\n",
      "4. 季節感\n",
      "\n",
      "5. 季節感\n",
      "\n",
      "6. 季節感\n",
      "\n",
      "7. 季節感\n",
      "\n",
      "8. 季節感\n",
      "\n",
      "9. 季節感\n",
      "\n",
      "10. 季節感\n",
      "\n",
      "11. 季節感\n",
      "\n",
      "12. 季節感\n",
      "\n",
      "13. 季節感\n",
      "\n",
      "14. 季節感\n",
      "\n",
      "15. 季節感\n",
      "\n",
      "16. 季節感\n",
      "\n",
      "17. 季節感\n",
      "\n",
      "18. 季節感\n",
      "\n",
      "19. 季節感\n",
      "\n",
      "20. 季節感\n",
      "\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 多言語対応モデル（日本語含む）- 小型のものを選択\n",
    "model_name = \"cyberagent/open-calm-small\"\n",
    "\n",
    "try:\n",
    "    # トークナイザーとモデルのロード\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cpu\")\n",
    "    \n",
    "    # 日本語のプロンプト\n",
    "    prompt = \"日本の四季について短い文章を書いてください。\\n\"\n",
    "    \n",
    "    # 入力のトークン化\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # テキスト生成\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=150,  # 短くして処理を高速化\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # 生成されたテキストのデコード\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading or using the model: {e}\")\n",
    "    print(\"\\nTrying with a different multilingual model...\")\n",
    "    \n",
    "    # 代替として別の多言語モデルを使用\n",
    "    from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "    \n",
    "    alt_model_name = \"facebook/mbart-large-50\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(alt_model_name)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(alt_model_name).to(\"cpu\")\n",
    "    \n",
    "    # 英語から日本語への翻訳例\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    encoded = tokenizer(\"The four seasons in Japan are beautiful.\", return_tensors=\"pt\")\n",
    "    \n",
    "    generated_tokens = model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"ja_XX\"]\n",
    "    )\n",
    "    \n",
    "    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    print(\"翻訳結果:\", translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ファインチューニング\n",
    "\n",
    "既存のモデルを特定のタスクに適応させるためのファインチューニングを紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 16168.94 examples/s]\n",
      "Training Epoch 1: 100%|██████████| 125/125 [00:56<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average training loss: 0.4873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7700\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 軽量なモデルを選択\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "dataset_name = \"sst2\"  # 感情分析データセット\n",
    "\n",
    "# データセットのロード\n",
    "dataset = load_dataset(dataset_name)\n",
    "train_dataset = dataset[\"train\"].select(range(1000))  # 訓練データを1000件に制限\n",
    "eval_dataset = dataset[\"validation\"].select(range(200))  # 評価データを200件に制限\n",
    "\n",
    "# トークナイザーとモデルのロード\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(\"cpu\")\n",
    "\n",
    "# データの前処理\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# データセットをトークン化\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_tokenized = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# PyTorchデータセットに変換\n",
    "train_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "eval_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# データローダーの作成\n",
    "train_dataloader = DataLoader(train_tokenized, batch_size=8, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_tokenized, batch_size=16)\n",
    "\n",
    "# オプティマイザーの設定\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# シンプルな訓練ループ\n",
    "num_epochs = 1\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 訓練ループ\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        # ラベルを取り出す\n",
    "        labels = batch.pop(\"label\").to(device)\n",
    "        # 残りの入力をモデルに渡す\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # モデルの順伝播\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # 損失計算\n",
    "        loss = F.cross_entropy(outputs.logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 逆伝播と最適化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # 評価ループ\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            # ラベルを取り出す\n",
    "            labels = batch.pop(\"label\").to(device)\n",
    "            # 残りの入力をモデルに渡す\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # 予測\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            # 正解数をカウント\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# モデルの保存\n",
    "model.save_pretrained(\"./finetuned-sentiment-model\")\n",
    "tokenizer.save_pretrained(\"./finetuned-sentiment-model\")\n",
    "\n",
    "# ファインチューニングしたモデルで予測\n",
    "test_text = \"I really enjoyed this movie, it was fantastic!\"\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "predicted_class = outputs.logits.argmax(-1).item()\n",
    "print(f\"Predicted class: {predicted_class}\")  # 1=positive、0=negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファインチューニング済みモデルの使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I really enjoyed this movie. The plot was excellent!\n",
      "Sentiment: LABEL_1, Score: 0.8711\n",
      "\n",
      "Sentence: The service at this restaurant was terrible and the food was bland.\n",
      "Sentiment: LABEL_0, Score: 0.9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ファインチューニングしたモデルをロード\n",
    "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\"./finetuned-sentiment-model\")\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./finetuned-sentiment-model\")\n",
    "\n",
    "# モデルを使って予測\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=fine_tuned_model, tokenizer=fine_tuned_tokenizer, device=\"cpu\")\n",
    "\n",
    "# 予測の実行\n",
    "test_sentences = [\n",
    "    \"I really enjoyed this movie. The plot was excellent!\",\n",
    "    \"The service at this restaurant was terrible and the food was bland.\"\n",
    "]\n",
    "\n",
    "results = sentiment_analysis(test_sentences)\n",
    "for sentence, result in zip(test_sentences, results):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hugging Face Hubとの連携\n",
    "\n",
    "Hugging Face Hubから既存のモデルを利用する方法を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "感情分析モデルを使用した例：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト: 'I really enjoyed this movie!'\n",
      "感情分析結果: POSITIVE, スコア: 0.9999\n",
      "--------------------------------------------------\n",
      "\n",
      "テキスト生成モデルを使用した例：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "プロンプト: 'Artificial intelligence will'\n",
      "生成テキスト: 'Artificial intelligence will soon become even easier to understand than artificial intelligence. AI can become the next smart person that I am. It will be able to'\n",
      "--------------------------------------------------\n",
      "\n",
      "穴埋めモデルを使用した例：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のテキスト: 'The goal of artificial intelligence is to [MASK] human tasks.'\n",
      "予測された穴埋め:\n",
      "1. 'the goal of artificial intelligence is to perform human tasks.' (スコア: 0.3227)\n",
      "2. 'the goal of artificial intelligence is to accomplish human tasks.' (スコア: 0.1006)\n",
      "3. 'the goal of artificial intelligence is to solve human tasks.' (スコア: 0.0902)\n",
      "--------------------------------------------------\n",
      "\n",
      "ローカルにダウンロード済みのモデルを使用する方法:\n",
      "\n",
      "# ダウンロード済みのモデルを使用\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "\n",
      "model_path = \"./finetuned-sentiment-model\"  # ローカルのパス\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
      "\n",
      "# モデルを使用\n",
      "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
      "result = classifier(\"I love this product!\")\n",
      "print(result)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1. 感情分析モデル \n",
    "try:\n",
    "    print(\"感情分析モデルを使用した例：\")\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\", \n",
    "                                model=\"distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "                                device=\"cpu\")\n",
    "    \n",
    "    text = \"I really enjoyed this movie!\"\n",
    "    result = sentiment_analyzer(text)\n",
    "    print(f\"テキスト: '{text}'\")\n",
    "    print(f\"感情分析結果: {result[0]['label']}, スコア: {result[0]['score']:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"感情分析モデルのロードエラー: {e}\")\n",
    "\n",
    "# 2. テキスト生成モデル \n",
    "try:\n",
    "    print(\"\\nテキスト生成モデルを使用した例：\")\n",
    "    model_name = \"distilgpt2\"\n",
    "    \n",
    "    # モデルとトークナイザーをロード\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # テキスト生成パイプライン\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=\"cpu\")\n",
    "    \n",
    "    # テキスト生成\n",
    "    prompt = \"Artificial intelligence will\"\n",
    "    result = generator(prompt, max_length=30, num_return_sequences=1)\n",
    "    \n",
    "    print(f\"プロンプト: '{prompt}'\")\n",
    "    print(f\"生成テキスト: '{result[0]['generated_text']}'\")\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"テキスト生成モデルのロードエラー: {e}\")\n",
    "\n",
    "# 3. 穴埋めモデル \n",
    "try:\n",
    "    print(\"\\n穴埋めモデルを使用した例：\")\n",
    "    unmasker = pipeline('fill-mask', model='distilbert-base-uncased', device=\"cpu\")\n",
    "    \n",
    "    text = \"The goal of artificial intelligence is to [MASK] human tasks.\"\n",
    "    results = unmasker(text)\n",
    "    \n",
    "    print(f\"元のテキスト: '{text}'\")\n",
    "    print(\"予測された穴埋め:\")\n",
    "    for i, result in enumerate(results[:3], 1):\n",
    "        print(f\"{i}. '{result['sequence']}' (スコア: {result['score']:.4f})\")\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"穴埋めモデルのロードエラー: {e}\")\n",
    "\n",
    "# 4. 代替として、すでにダウンロード済みのモデルを使用する方法\n",
    "print(\"\\nローカルにダウンロード済みのモデルを使用する方法:\")\n",
    "print(\"\"\"\n",
    "# ダウンロード済みのモデルを使用\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_path = \"./finetuned-sentiment-model\"  # ローカルのパス\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# モデルを使用\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "result = classifier(\"I love this product!\")\n",
    "print(result)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "多くのHugging Faceはモデルを簡単に使用できることが分かったかと思います。大きなモデルではなく小さなモデルを選んでも、バッチサイズやシーケンス長を調整することで、限られたリソースでも十分な性能を発揮できます。\n",
    "\n",
    "さらに学習を深めたい場合は、[Hugging Face Hub](https://huggingface.co/models)で「distil」や「small」などのキーワードで検索して、軽量モデルを探してみることをお勧めします。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
